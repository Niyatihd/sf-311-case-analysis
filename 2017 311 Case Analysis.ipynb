{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sodapy import Socrata\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import nan as NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our `client` to connect to the DataSF API. For requests to the API for 311 data alone, I've found that a Socrata app token is unnecessary (I was not throttled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Socrata(\"data.sfgov.org\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource ID for 311 data\n",
    "socrata_resource_id = \"ktji-gk7t\"\n",
    "\n",
    "start_date = '2017-01-01T00:00:00'\n",
    "end_date = '2018-01-01T00:00:00'\n",
    "record_limit=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.get(socrata_resource_id, \n",
    "                     where=f\"requested_datetime between '{start_date}' and '{end_date}'\", \n",
    "                     limit=record_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploration & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, with DataFrames of many columns, I like to keep a list of columns for review, popping columns off as I validate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review = list(df.columns)\n",
    "columns_to_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to drop\n",
    "\n",
    "`point` looks like GeoJSON data, and doesn't convey any more information for our analysis than `lat` and `long`. Working with GeoJSON data here will be difficult, so we drop it.\n",
    "\n",
    "For this analysis, we don't care about `media_url` (the URL of any image, for instance, attached to the case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.drop(columns=['point', 'media_url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review.remove('point')\n",
    "columns_to_review.remove('media_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate records in the 311 data, at least when comparing the exact values of fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a few fields for which we have missing data:\n",
    "\n",
    "* I'd expect missing `closed_date` to indicate that the case is still open. We should exclude some of these from analysis, but this is expected.\n",
    "* Other fields, e.g. `lat`, `long`, and `police_district`, are all related. If we're missing a (lat, long), this means these other fields likely aren't populated. Let's confirm that the records for which these values are missing are _all_ missing.\n",
    "\n",
    "**Let's go through each column and review the data within to confirm there aren't other gotchas hiding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-related features\n",
    "\n",
    "`police_district`, `neighborhoods_sffind_boundaries` and `supervisor_district` are all related to the lat and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.lat == '0') | (df.long == '0')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.lat == '0') & df.police_district.isnull() \n",
    "   & df.neighborhoods_sffind_boundaries.isnull() \n",
    "   & df.supervisor_district.isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most records with a (lat, long) of 0 are also missing the associated geographic dimensions.**\n",
    "\n",
    "For questions unrelated to geography, this doesn't matter, so let's move on.\n",
    "\n",
    "Next, we check for missing values that appear as empty strings (the checks above would identify only `NA` values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.neighborhoods_sffind_boundaries.str.len() == 0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run this check quite a bit on other fields, so let's abstract this as its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_emtpy_strings(df, field):\n",
    "    return df[df[field].str.len() == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_emtpy_strings(df, 'neighborhoods_sffind_boundaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.neighborhoods_sffind_boundaries.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 neighborhoods by 311 calls of any type\n",
    "# This isn't normalized by population, but gives us a sense of call volume\n",
    "df.neighborhoods_sffind_boundaries.value_counts().head(10).iloc[::-1].plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_emtpy_strings(df, 'police_district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `supervisor_district`\n",
    "\n",
    "In addition to other data-cleanliness tasks, I'm also interested to see whether all 311 calls have a valid supervisor district attached. We can pull in the list of current supervisor districts from `DataSF`, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_district_sfdata_resource_id = 'keex-zmn4'\n",
    "supervisor_district_data = client.get(supervisor_district_sfdata_resource_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_district_df = DataFrame.from_records(supervisor_district_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_districts = set(supervisor_district_df.supervisor.unique())\n",
    "supervisor_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The - operator lets us find the difference between two sets\n",
    "invalid_supervisor_districts = set(df.supervisor_district.unique()) - supervisor_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.supervisor_district.isin(invalid_supervisor_districts)].supervisor_district.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see roughly 1,000 records with a supervisor district of 0. If we dig into supervisor district later, let's take note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review.remove('lat')\n",
    "columns_to_review.remove('long')\n",
    "columns_to_review.remove('neighborhoods_sffind_boundaries')\n",
    "columns_to_review.remove('supervisor_district')\n",
    "columns_to_review.remove('police_district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `address`\n",
    "\n",
    "There is only 1 record in this dataset where address is null, and no records where address is an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.address.isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_emtpy_strings(df, 'address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.address.str.len() < 20].address.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 records with an address of `Tara Boss`. I'm unsure what this means, but it's a small issue. Let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review.remove('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agency_responsible`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_emtpy_strings(df, 'agency_responsible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.agency_responsible.isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agency_responsible.value_counts().head(10).iloc[::-1].plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agency_responsible` field seems to be relatively well-maintained, and doesn't appear to be missing data. This does not mean we won't find issues with it later, but let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review.remove('agency_responsible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `requested_datetime`, `closed_date` and `updated_datetime`\n",
    "\n",
    "For some analyses, we want to keep tickets that are not closed, so we keep missing data from this field.\n",
    "\n",
    "First, we want to convert all fields to datetime types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.requested_datetime = pd.to_datetime(df.requested_datetime)\n",
    "df.closed_date = pd.to_datetime(df.closed_date)\n",
    "df.updated_datetime = pd.to_datetime(df.updated_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a sense for when tickets are opened and closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = df.requested_datetime.dt.month.unique()\n",
    "opened = df.requested_datetime.dt.month.value_counts().sort_index()\n",
    "closed = df.closed_date.dt.month.value_counts().sort_index()\n",
    "\n",
    "plt.plot(x, opened, color='blue')\n",
    "plt.plot(x, closed, color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of tickets are still open? Are most of these just opened in December, or are there others that remain open for some reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_tickets = df[df.closed_date.isnull()]\n",
    "open_tickets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_tickets.requested_datetime.dt.month.value_counts().sort_index().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot more we could investigate here, but let's move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_review.remove('requested_datetime')\n",
    "columns_to_review.remove('closed_date')\n",
    "columns_to_review.remove('updated_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I like to save the filtered / cleaned DataFrame to a CSV that we can use in the rest of our analysis (typically done in a separate notebook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
